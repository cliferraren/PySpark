{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP - CountVector Implementation of Airline Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, StopWordsRemover, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the SparkSession\n",
    "spark = SparkSession.builder.appName(\"countVector\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      Airline Tweets|\n",
      "+--------------------+\n",
      "|@VirginAmerica pl...|\n",
      "|@VirginAmerica se...|\n",
      "|@VirginAmerica do...|\n",
      "|@VirginAmerica Ar...|\n",
      "|@VirginAmerica aw...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read in the CSV file\n",
    "\n",
    "dataframe = spark.read.format(\"csv\").option(\"header\",\"True\").load(\"data/airlines.csv\")\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|      Airline Tweets|               words|\n",
      "+--------------------+--------------------+\n",
      "|@VirginAmerica pl...|[@virginamerica, ...|\n",
      "|@VirginAmerica se...|[@virginamerica, ...|\n",
      "|@VirginAmerica do...|[@virginamerica, ...|\n",
      "|@VirginAmerica Ar...|[@virginamerica, ...|\n",
      "|@VirginAmerica aw...|[@virginamerica, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Tokenize Dataframe\n",
    "tokened = Tokenizer(inputCol='Airline Tweets', outputCol='words')\n",
    "tokened_transformed = tokened.transform(dataframe)\n",
    "tokened_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|      Airline Tweets|               words|            filtered|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|@VirginAmerica pl...|[@virginamerica, ...|[plus, you've, ad...|\n",
      "|@VirginAmerica se...|[@virginamerica, ...|[seriously, would...|\n",
      "|@VirginAmerica do...|[@virginamerica, ...|[do, you, miss, m...|\n",
      "|@VirginAmerica Ar...|[@virginamerica, ...|[are, the, hours,...|\n",
      "|@VirginAmerica aw...|[@virginamerica, ...|[awaiting, my, re...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#remove Stop Words\n",
    "stop_list = [\"@VirginAmerica\", \"$30\", \"@virginamerica\"]\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered', stopWords=stop_list)\n",
    "removed_df = remover.transform(tokened_transformed)\n",
    "removed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|      Airline Tweets|               words|            filtered|             Vectors|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|@VirginAmerica pl...|[@virginamerica, ...|[plus, you've, ad...|(20,[0,3,9,14],[1...|\n",
      "|@VirginAmerica se...|[@virginamerica, ...|[seriously, would...|(20,[0,1,4,5,7,8,...|\n",
      "|@VirginAmerica do...|[@virginamerica, ...|[do, you, miss, m...|     (20,[13],[1.0])|\n",
      "|@VirginAmerica Ar...|[@virginamerica, ...|[are, the, hours,...|(20,[0,1,2,5,6,16...|\n",
      "|@VirginAmerica aw...|[@virginamerica, ...|[awaiting, my, re...|(20,[2,3,4,10,11,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Vectorized the Term Frequency\n",
    "counvectorizer = CountVectorizer(minTF=1.0, minDF=1.0, vocabSize=20, inputCol='filtered', \n",
    "                                 outputCol='Vectors')\n",
    "\n",
    "model = counvectorizer.fit(removed_df)\n",
    "result = model.transform(removed_df)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit data with the IDF\n",
    "idf = IDF(inputCol='Vectors', outputCol='Features,[Indexes],[TF_IDF]')\n",
    "idfModel = idf.fit(result)\n",
    "rescaledData = idfModel.transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------+\n",
      "|            filtered|Features,[Indexes],[TF_IDF]|\n",
      "+--------------------+---------------------------+\n",
      "|[plus, you've, ad...|       (20,[0,3,9,14],[0...|\n",
      "|[seriously, would...|       (20,[0,1,4,5,7,8,...|\n",
      "|[do, you, miss, m...|       (20,[13],[1.09861...|\n",
      "|[are, the, hours,...|       (20,[0,1,2,5,6,16...|\n",
      "|[awaiting, my, re...|       (20,[2,3,4,10,11,...|\n",
      "+--------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the dataframe\n",
    "rescaledData.select('filtered', 'Features,[Indexes],[TF_IDF]').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        terms|\n",
      "+-------------+\n",
      "|         plus|\n",
      "|       you've|\n",
      "|        added|\n",
      "|  commercials|\n",
      "|           to|\n",
      "|          the|\n",
      "|experience...|\n",
      "|       tacky.|\n",
      "|    seriously|\n",
      "|        would|\n",
      "|          pay|\n",
      "|            a|\n",
      "|       flight|\n",
      "|          for|\n",
      "|        seats|\n",
      "|         that|\n",
      "|       didn't|\n",
      "|         have|\n",
      "|         this|\n",
      "|     playing.|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#From the tokenized dataframe grab every word used.\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "df_vocab = removed_df.select('filtered').rdd.\\\n",
    "        flatMap(lambda x: x[0]).\\\n",
    "        toDF(schema=StringType()).toDF('terms')\n",
    "df_vocab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use string indexer to of each word\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringindexer = StringIndexer(inputCol='terms', outputCol='StringIndexer(index)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|   terms|StringIndexer(index)|\n",
      "+--------+--------------------+\n",
      "|     the|                 0.0|\n",
      "|     for|                 1.0|\n",
      "|     are|                 2.0|\n",
      "|   would|                 3.0|\n",
      "|  online|                 4.0|\n",
      "|    that|                 5.0|\n",
      "|      to|                 6.0|\n",
      "|    it's|                 7.0|\n",
      "|current?|                 8.0|\n",
      "|    this|                 9.0|\n",
      "|    have|                10.0|\n",
      "|    your|                11.0|\n",
      "|   worry|                12.0|\n",
      "|  didn't|                13.0|\n",
      "|    plus|                14.0|\n",
      "|  flight|                15.0|\n",
      "|awaiting|                16.0|\n",
      "|  return|                17.0|\n",
      "|      do|                18.0|\n",
      "|    club|                19.0|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fit the string indexer to the vocab set and remove duplicates\n",
    "stringindexer.fit(df_vocab).transform(df_vocab).distinct().orderBy('StringIndexer(index)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
